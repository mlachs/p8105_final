---
title: "fp2513"
output: github_document
date: "2024-11-07"
---

```{r setup, include=FALSE}
library(tidyverse)
```

```{r}
library(httr)
library(jsonlite)


res=GET('https://developer.nps.gov/api/v1/newsreleases?limit=10000&api_key=B9nDpbkbrb3kSOjz6kXSxMJ3d6MSpUvt1QqYdeyn')



data = res %>%  content("text") %>%  jsonlite::fromJSON() %>%  as_tibble()
```

```{r}
data %>% 
  janitor::clean_names() %>% 
  select(data) %>% 
  unnest(data) %>% 
  view()
```

Look at newreleases 
Looking potentially at coverage across the years see how the distribution of coverage changes if it does (map visualisation)

Topics covered each year (overall and within regions), text analysis

Historic traffic counts to see how it correlates to boom in automobiles?
Public use Statistics and how that changes over time
With the 

Text Analysis of news releases
```{r}
library(tm)
library(tidytext)
library(stringr)
library(dplyr)
library(ggplot2)

text_data = data %>% 
  janitor::clean_names() %>% 
  select(data) %>% 
  unnest(data) %>% 
  select(title, abstract, relatedParks, releaseDate)

clean_text = function(text) {
  text <- tolower(text)
  text <- removePunctuation(text)
  text <- removeNumbers(text)
  text <- stripWhitespace(text)
  text <- removeWords(text, stopwords("en"))
  return(text)
}

text_data$cleaned_title = sapply(text_data$title, clean_text)

tidy_titles = text_data %>%
  unnest_tokens(word, cleaned_title)
```

```{r}
library(wordcloud)

word_count = tidy_titles %>%
  count(word, sort = TRUE)

wordcloud(words = word_count$word, freq = word_count$n, min.freq = 20)
```

```{r}
sentiments = tidy_titles %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE)

sentiments %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(x = reorder(word, n), y = n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  facet_wrap(~sentiment, scales = "free_y") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Top Words by Sentiment in Titles", x = "Words", y = "Frequency")
```

```{r}
library(topicmodels)
library(tm)
library(reshape2)

docu_term_matrix = DocumentTermMatrix(VCorpus(VectorSource(text_data$cleaned_title)))

lda_model = LDA(docu_term_matrix, k = 3, control = list(seed = 1234))
topics = tidy(lda_model, matrix = "beta")

top_terms = topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms

```


Historic Traffic Data

```{r}
his_traffic_df = read_csv(file = "data/(east) Query Builder for Traffic Counts (1985 - Last Calendar Year).csv", 
                           na = c("NA", ",", ".")) %>% 
  janitor::clean_names() %>% 
  filter(park_name %in% c("Fire Island NS", "Gateway NRA", "Saratoga NHP"))

ggplot(his_traffic_df, aes(x = year, y = traffic_count_total, color = traffic_counter)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE) +
  labs(title = "Traffic Count Totals by Year for Selected Parks in New York",
       x = "Year",
       y = "Total Traffic Count",
       color = "Traffic Counter") +
  theme_minimal() +
  facet_wrap(~ park_name, scales = "free_y")

```


```{r}
his_traffic_df = 
  read_csv(file = "data/Query Builder for Traffic Counts (1985 - Last Calendar Year).csv", na = c("NA", ",", ".")) %>% 
  janitor::clean_names() 

his_traffic_df %>% 
  select(park_name) %>% 
  distinct() %>% 
  pull()
print(unique_park_names)
```


Compare east and west coast trends specifically between New York and California which is really popular with natioanl parks looking at traffic, visitation trends historically 

What can we gain knowledge about?

Maybe even create a map in the end of america that plots interactive points of the parks and plotly will show you
The name of the park, year with most traffic and count and year with least traffic and count 
regression on expected traffic ? whether we expect to go down or up 
how many traffic cameras 


Help us understand traffic distribution better traffic laws and if we need to do anything about helping the roads 



```{r}
geo_coord_df = 
  read_csv(file = "data/NPS_-_Points_of_Interest_(POIs)_-_Geographic_Coordinate_System.csv", na = c("NA", ",", ".")) %>% 
  janitor::clean_names() %>% 
  select(x, y, poiname, maplabel, poitype, unitname, seasonal, seasdesc) %>%
  filter(poitype %in% c('Parking Lot','Parking - Disabled', 'Driving Tour', 'Electric Vehicle Parking', 'Four-wheel Drive Trail'))

```


```{r}
library(plotly)

poitype_colors = c("Parking Lot" = "red", 
                    "Parking - Disabled" = "green",
                    "Driving Tour" = "violet", 
                    "Electric Vehicle Parking" = "gold",
                    "Four-wheel Drive Trail" = "blue")

geo_coord_df$color = poitype_colors[geo_coord_df$poitype]

plot_ly(data = geo_coord_df) %>%
  add_trace(
    type = "scattergeo",
    mode = "markers",
    lon = ~x,
    lat = ~y,
    text = ~paste(maplabel, "<br>", unitname),
    marker = list(
      size = 10,
      opacity = 0.7,
      color = ~color  
    )
  ) %>%
  layout(
    geo = list(
      projection = list(type = 'albers usa'),  
      scope = 'usa',  
      showland = TRUE,
      landcolor = 'lightgrey',  
      countrycolor = 'white',
    title = "Locations on the Map of America"
  ))
```


Colored ggplot based on state and count of parking lots per state

```{r}
library(tigris)
library(sf)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(viridis)

# Convert geo_coord_df to spatial dataframe with latitude/longitude coordinates
map_geo_coord_df = st_as_sf(geo_coord_df, coords = c("x", "y"), crs = 4326)

# Get US states shapefile (boundary) and ensure CRS is correct
US_states = states(cb = TRUE)
US_states_sf = st_as_sf(US_states)

# Transform coordinate reference system (if necessary)
US_states_sf = st_transform(US_states_sf, crs = 4326)

# Spatial join between parking lot coordinates and US states polygons
map_geo_coord_sf = st_join(map_geo_coord_df, US_states_sf)

# Filter for Parking Lots and remove missing state names
map_geo_coord_sf = map_geo_coord_sf %>%
  filter(poitype == "Parking Lot") %>%
  filter(!is.na(NAME))

# Count parking lots per state
parking_lots_per_state = map_geo_coord_sf %>%
  group_by(NAME) %>%
  summarise(parking_lots = n(), .groups = "drop")

# Now we need to merge parking_lots_per_state with US_states_sf using st_join
# First, convert parking_lots_per_state to a spatial object with the same CRS as US_states_sf
# Create a dummy geometry for the parking_lots_per_state data (this will allow the spatial join)
parking_lots_sf = parking_lots_per_state %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  st_transform(crs = st_crs(US_states_sf))  # Make sure the CRS matches

# Now we can join parking_lots_sf with US_states_sf using st_join
US_states_sf = st_join(US_states_sf, parking_lots_sf, left = TRUE)

# Plot the map with parking lots colored by state
ggplot(data = US_states_sf) +
  geom_sf(aes(fill = parking_lots), color = "black") +  # Color states by parking lot count
  scale_fill_viridis_c(option = "plasma", na.value = "grey90", name = "Parking Lots") +  # Color scale
  theme_minimal() +  # Clean theme
  labs(title = "Number of Parking Lots in National Parks per State",
       subtitle = "States Colored by Parking Lot Count") +
  theme(
    legend.position = "right",  # Position the legend
    plot.title = element_text(size = 20, face = "bold"),
    plot.subtitle = element_text(size = 15)
  ) +
  coord_sf(
    xlim = c(-125, -65),  # Set longitude limits for the continental US
    ylim = c(24, 50),     # Set latitude limits for the mainland US
    expand = FALSE         # Don't expand the limits beyond the state boundaries
  )

```


Interactive Plotly that shows state name and count parking lots below

```{r}
library(tigris)
library(sf)
library(dplyr)
library(viridis)
library(plotly)

# Convert geo_coord_df to spatial dataframe with latitude/longitude coordinates
map_geo_coord_df = st_as_sf(geo_coord_df, coords = c("x", "y"), crs = 4326)

# Get US states shapefile (boundary) and ensure CRS is correct
US_states = states(cb = TRUE)
US_states_sf = st_as_sf(US_states)

# Transform coordinate reference system (if necessary)
US_states_sf = st_transform(US_states_sf, crs = 4326)

# Spatial join between parking lot coordinates and US states polygons
map_geo_coord_sf = st_join(map_geo_coord_df, US_states_sf, join = st_within)

# Filter for Parking Lots and remove missing state names
map_geo_coord_sf = map_geo_coord_sf %>%
  filter(poitype == "Parking Lot") %>%
  filter(!is.na(NAME))

# Count parking lots per state
parking_lots_per_state = map_geo_coord_sf %>%
  group_by(NAME) %>%
  summarise(parking_lots = n(), .groups = "drop")

# Convert parking_lots_per_state to a regular dataframe (no geometry)
parking_lots_per_state_df = as.data.frame(parking_lots_per_state)

# Merge the parking_lots_per_state data with US_states_sf using left_join
US_states_sf = US_states_sf %>%
  left_join(parking_lots_per_state_df, by = "NAME")

# Prepare the data for plotting with plotly
US_states_sf_df = as.data.frame(US_states_sf)
US_states_sf_df$geometry = NULL  # Remove the geometry for plotly

# Check if NAME column is present
print(names(US_states_sf_df))

# Create an interactive plotly map
plot_ly(data = US_states_sf_df, type = "choropleth",
        locations = ~STUSPS, locationmode = "USA-states",
        z = ~parking_lots, text = ~paste("State:", NAME, "<br>Parking Lots:", parking_lots),
        colorscale = "Viridis", reversescale = TRUE,
        marker = list(line = list(color = "black", width = 0.5))) %>%
  colorbar(title = "Parking Lots") %>%
  layout(title = "Number of Parking Lots in National Parks per State",
         geo = list(scope = 'usa',
                    projection = list(type = 'albers usa'),
                    showlakes = TRUE,
                    lakecolor = toRGB('white')))

```



```{r}

# Convert geo_coord_df to spatial dataframe with latitude/longitude coordinates
map_geo_coord_df = st_as_sf(geo_coord_df, coords = c("x", "y"), crs = 4326)

# Get US states shapefile (boundary) and ensure CRS is correct
US_states = states(cb = TRUE)
US_states_sf = st_as_sf(US_states)

# Transform coordinate reference system (if necessary)
US_states_sf = st_transform(US_states_sf, crs = 4326)

# Spatial join between geo_coord_df and US states polygons
map_geo_coord_sf = st_join(map_geo_coord_df, US_states_sf)

# Filter for "Parking Lot" and remove rows with missing state names
map_geo_coord_sf = map_geo_coord_sf %>%
  filter(poitype == "Parking Lot") %>%
  filter(!is.na(NAME))  # Ensure no missing state names

# Count unique national parks per state (based on unique unitname)
national_parks_per_state = map_geo_coord_sf %>%
  group_by(NAME) %>%
  summarise(national_parks = n_distinct(unitname), .groups = "drop")

# Count the number of parking lots per state
parking_lots_per_state = map_geo_coord_sf %>%
  group_by(NAME) %>%
  summarise(parking_lots = n(), .groups = "drop")

# Merge the parking lots and national parks counts into one regular dataframe
state_counts = left_join(parking_lots_per_state, national_parks_per_state, by = "NAME") %>%
  mutate(ratio = parking_lots / national_parks)  # Calculate the ratio of parking lots to national parks

# Convert the state_counts dataframe into a spatial object (no geometry, just the counts)
state_counts_sf = st_as_sf(state_counts, coords = c("x", "y"), crs = 4326)

# Now, merge the ratio data with the spatial states data using st_join
# Ensure we don't lose the geometry during this join
US_states_sf = st_join(US_states_sf, state_counts, by = "NAME")

# Plot the map with states colored by the parking lot to national park ratio
ggplot(data = US_states_sf) +
  geom_sf(aes(fill = ratio), color = "black") +  # Color states by the ratio
  scale_fill_viridis_c(option = "plasma", na.value = "grey90", name = "Parking Lot to National Park Ratio") +  # Color scale
  theme_minimal() +  # Clean theme
  labs(title = "Ratio of Parking Lots to National Parks per State",
       subtitle = "States Colored by Ratio of Parking Lots to National Parks") +
  theme(
    legend.position = "right",  # Position the legend
    plot.title = element_text(size = 20, face = "bold"),
    plot.subtitle = element_text(size = 15)
  ) +
  coord_sf(
    xlim = c(-125, -65),  # Set longitude limits for the continental US
    ylim = c(24, 50),     # Set latitude limits for the mainland US
    expand = FALSE         # Don't expand the limits beyond the state boundaries
  )


```


If want to explore the traffic, issue is that there are many different types of national parks. Some that will inherently need more parking / traffic than others. But that is hard to quantify. 

Attempt to do it on a smaller scale than instead the whole of america 


```{r}
read_csv(file = "data/(east) Query Builder for Traffic Counts (1985 - Last Calendar Year).csv", 
                           na = c("NA", ",", ".")) %>% 
  janitor::clean_names() %>% 
  distinct(park_name_total) %>% 
  print(n = 34)
```

```{r}
# Ensure all parks have corresponding states
park_to_state = tibble::tibble(
  park_name_total = c(
    "Acadia NP", "Allegheny Portage Railroad NHS", "Appomattox Court House NHP", 
    "Assateague Island NS", "Booker T. Washington NM", "Cape Cod NS", 
    "Colonial NHP", "Delaware Water Gap NRA", "Fire Island NS", 
    "Fort McHenry NM & HS", "Fort Necessity NB", "Fredericksburg & Spotsylvania NMP", 
    "Gateway NRA", "Gauley River NRA", "George Washington Birthplace NM", 
    "Gettysburg NMP", "Home of Franklin D. Roosevelt NHS", "Hopewell Furnace NHS", 
    "Johnstown Flood NMEM", "Katahdin Woods and Waters NM", 
    "Martin Van Buren NHS", "Minute Man NHP", "Morristown NHP", 
    "New River Gorge NP & PRES", "Paterson Great Falls NHP", "Petersburg NB", 
    "Richmond NBP", "Saint-Gaudens NHP", "Saratoga NHP", "Shenandoah NP", 
    "Steamtown NHS", "Upper Delaware S&RR", "Valley Forge NHP", 
    "Vanderbilt Mansion NHS"
  ),
  state = c(
    "ME", "PA", "VA", "MD", "VA", "MA", "VA", "NJ", "NY", "MD", "PA", "VA", 
    "WV", "WV", "VA", "PA", "NY", "PA", "PA", "PA", "ME", "NY", "MA", "NJ", 
    "VA", "VA", "NH", "NY", "VA", "PA", "PA", "PA", "NY", "PA"  # Add the missing state(s) here
  )
)

# Check that lengths match
length(park_to_state$park_name_total)
length(park_to_state$state)


traffic_counts_df = his_traffic_df %>%
  left_join(park_to_state, by = "park_name_total")

# Check the result
head(traffic_counts_df)

```

```{r}
library(dplyr)
library(ggplot2)

# Calculate median traffic_count_total for each state and year
mean_traffic_counts_by_state_year = traffic_counts_df %>%
  group_by(state, year) %>%
  summarise(mean_traffic_count = mean(traffic_count_total, na.rm = TRUE), .groups = "drop")

# Plot the data with faceting by state
ggplot(mean_traffic_counts_by_state_year, aes(x = year, y = mean_traffic_count)) +
  geom_line() +
  facet_wrap(~ state, scales = "free_y") +
  theme_minimal() +
  labs(title = "Mean Traffic Count by State and Year",
       x = "Year",
       y = "Mean Traffic Count") +
  theme(
    strip.text = element_text(size = 10, face = "bold"),
    plot.title = element_text(size = 16, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```


