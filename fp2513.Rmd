---
title: "fp2513"
output: github_document
date: "2024-11-07"
---

```{r setup, include=FALSE}
library(tidyverse)
```

```{r}
library(httr)
library(jsonlite)


res=GET('https://developer.nps.gov/api/v1/newsreleases?limit=10000&api_key=B9nDpbkbrb3kSOjz6kXSxMJ3d6MSpUvt1QqYdeyn')



data = res %>%  content("text") %>%  jsonlite::fromJSON() %>%  as_tibble()
```

```{r}
data %>% 
  janitor::clean_names() %>% 
  select(data) %>% 
  unnest(data) %>% 
  view()
```

Look at newreleases 
Looking potentially at coverage across the years see how the distribution of coverage changes if it does (map visualisation)

Topics covered each year (overall and within regions), text analysis

Historic traffic counts to see how it correlates to boom in automobiles?
Public use Statistics and how that changes over time
With the 

Text Analysis of news releases
```{r}
library(tm)
library(tidytext)
library(stringr)
library(dplyr)
library(ggplot2)

text_data = data %>% 
  janitor::clean_names() %>% 
  select(data) %>% 
  unnest(data) %>% 
  select(title, abstract, relatedParks, releaseDate)

clean_text = function(text) {
  text <- tolower(text)
  text <- removePunctuation(text)
  text <- removeNumbers(text)
  text <- stripWhitespace(text)
  text <- removeWords(text, stopwords("en"))
  return(text)
}

text_data$cleaned_title = sapply(text_data$title, clean_text)

tidy_titles = text_data %>%
  unnest_tokens(word, cleaned_title)
```

```{r}
library(wordcloud)

word_count = tidy_titles %>%
  count(word, sort = TRUE)

wordcloud(words = word_count$word, freq = word_count$n, min.freq = 20)
```

```{r}
sentiments = tidy_titles %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE)

sentiments %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(x = reorder(word, n), y = n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  facet_wrap(~sentiment, scales = "free_y") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Top Words by Sentiment in Titles", x = "Words", y = "Frequency")
```

```{r}
library(topicmodels)
library(tm)
library(reshape2)

docu_term_matrix = DocumentTermMatrix(VCorpus(VectorSource(text_data$cleaned_title)))

lda_model = LDA(docu_term_matrix, k = 3, control = list(seed = 1234))
topics = tidy(lda_model, matrix = "beta")

top_terms = topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms

```


Historic Traffic Data

```{r}
his_traffic_df = read_csv(file = "data/(east) Query Builder for Traffic Counts (1985 - Last Calendar Year).csv", 
                           na = c("NA", ",", ".")) %>% 
  janitor::clean_names() %>% 
  filter(park_name %in% c("Fire Island NS", "Gateway NRA", "Saratoga NHP"))

ggplot(his_traffic_df, aes(x = year, y = traffic_count_total, color = traffic_counter)) +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE) +
  labs(title = "Traffic Count Totals by Year for Selected Parks in New York",
       x = "Year",
       y = "Total Traffic Count",
       color = "Traffic Counter") +
  theme_minimal() +
  facet_wrap(~ park_name, scales = "free_y")

```


```{r}
his_traffic_df = 
  read_csv(file = "data/Query Builder for Traffic Counts (1985 - Last Calendar Year).csv", na = c("NA", ",", ".")) %>% 
  janitor::clean_names() 

his_traffic_df %>% 
  select(park_name) %>% 
  distinct() %>% 
  pull()
print(unique_park_names)
```


Compare east and west coast trends specifically between New York and California which is really popular with natioanl parks looking at traffic, visitation trends historically 

What can we gain knowledge about?

Maybe even create a map in the end of america that plots interactive points of the parks and plotly will show you
The name of the park, year with most traffic and count and year with least traffic and count 
regression on expected traffic ? whether we expect to go down or up 
how many traffic cameras 


Help us understand traffic distribution better traffic laws and if we need to do anything about helping the roads 



```{r}
geo_coord_df = 
  read_csv(file = "data/NPS_-_Points_of_Interest_(POIs)_-_Geographic_Coordinate_System.csv", na = c("NA", ",", ".")) %>% 
  janitor::clean_names() %>% 
  select(x, y, poiname, maplabel, poitype, unitname, seasonal, seasdesc) %>%
  filter(poitype %in% c('Parking Lot','Parking - Disabled', 'Driving Tour', 'Electric Vehicle Parking', 'Four-wheel Drive Trail'))

```


```{r}
library(plotly)

poitype_colors = c("Parking Lot" = "red", 
                    "Parking - Disabled" = "green",
                    "Driving Tour" = "violet", 
                    "Electric Vehicle Parking" = "gold",
                    "Four-wheel Drive Trail" = "blue")

geo_coord_df$color = poitype_colors[geo_coord_df$poitype]

plot_ly(data = geo_coord_df) %>%
  add_trace(
    type = "scattergeo",
    mode = "markers",
    lon = ~x,
    lat = ~y,
    text = ~paste(maplabel, "<br>", unitname),
    marker = list(
      size = 10,
      opacity = 0.7,
      color = ~color  
    )
  ) %>%
  layout(
    geo = list(
      projection = list(type = 'albers usa'),  
      scope = 'usa',  
      showland = TRUE,
      landcolor = 'lightgrey',  
      countrycolor = 'white',
    title = "Locations on the Map of America"
  ))
```


```{r}
library(tigris)
library(sf)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(viridis)

# Convert geo_coord_df to spatial dataframe with latitude/longitude coordinates
map_geo_coord_df = st_as_sf(geo_coord_df, coords = c("x", "y"), crs = 4326)

# Get US states shapefile (boundary) and ensure CRS is correct
US_states = states(cb = TRUE)
US_states_sf = st_as_sf(US_states)

# Transform coordinate reference system (if necessary)
US_states_sf = st_transform(US_states_sf, crs = 4326)

# Spatial join between parking lot coordinates and US states polygons
map_geo_coord_sf = st_join(map_geo_coord_df, US_states_sf)

# Filter for Parking Lots and remove missing state names
map_geo_coord_sf = map_geo_coord_sf %>%
  filter(poitype == "Parking Lot") %>%
  filter(!is.na(NAME))

# Count parking lots per state
parking_lots_per_state = map_geo_coord_sf %>%
  group_by(NAME) %>%
  summarise(parking_lots = n(), .groups = "drop")

# Now we need to merge parking_lots_per_state with US_states_sf using st_join
# First, convert parking_lots_per_state to a spatial object with the same CRS as US_states_sf
# Create a dummy geometry for the parking_lots_per_state data (this will allow the spatial join)
parking_lots_sf = parking_lots_per_state %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>%
  st_transform(crs = st_crs(US_states_sf))  # Make sure the CRS matches

# Now we can join parking_lots_sf with US_states_sf using st_join
US_states_sf = st_join(US_states_sf, parking_lots_sf, left = TRUE)

# Plot the map with parking lots colored by state
ggplot(data = US_states_sf) +
  geom_sf(aes(fill = parking_lots), color = "black") +  # Color states by parking lot count
  scale_fill_viridis_c(option = "plasma", na.value = "grey90", name = "Parking Lots") +  # Color scale
  theme_minimal() +  # Clean theme
  labs(title = "Number of Parking Lots per State",
       subtitle = "States Colored by Parking Lot Count") +
  theme(
    legend.position = "right",  # Position the legend
    plot.title = element_text(size = 20, face = "bold"),
    plot.subtitle = element_text(size = 15)
  ) +
  coord_sf(
    xlim = c(-125, -65),  # Set longitude limits for the continental US
    ylim = c(24, 50),     # Set latitude limits for the mainland US
    expand = FALSE         # Don't expand the limits beyond the state boundaries
  )


```

